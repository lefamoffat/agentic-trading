# RL Agent Configuration for EUR/USD Trading

# PPO Configuration
ppo:
    policy: "MlpPolicy"
    learning_rate: 0.0003
    n_steps: 2048
    batch_size: 64
    n_epochs: 10
    gamma: 0.99
    gae_lambda: 0.95
    clip_range: 0.2
    ent_coef: 0.01
    vf_coef: 0.5
    max_grad_norm: 0.5
    normalize_advantage: True
    use_sde: False
    target_kl: null

# A3C Configuration
a3c:
    learning_rate: 0.0001
    gamma: 0.99
    gae_lambda: 1.0
    entropy_coef: 0.01
    value_loss_coef: 0.5
    max_grad_norm: 0.5
    n_workers: 4
    episodes: 1000
    ent_coef: "auto"
    target_update_interval: 1

# SAC Configuration
sac:
    learning_rate: 0.0003
    buffer_size: 1000000
    learning_starts: 100
    batch_size: 256
    tau: 0.005
    gamma: 0.99
    train_freq: 1
    gradient_steps: 1
    ent_coef: "auto"
    target_update_interval: 1
    episodes: 1000

# Training Configuration
training:
    eval_frequency: 1000 # Evaluate the model every 1000 steps
    n_eval_episodes: 5 # Run 5 episodes for each evaluation

# Model Selection
auto_selection:
    enabled: true
    algorithms: ["ppo", "sac", "a3c"]
    hyperopt_trials: 50
    cv_folds: 3

# Hyperparameter Optimization (HPO) search space for Optuna
hpo_params:
    # For PPO, we can tune these common hyperparameters
    ppo:
        # Learning rate will be sampled from a log uniform distribution
        learning_rate:
            type: "log_float"
            low: 0.00001
            high: 0.001

        # Number of steps to run for each environment per update
        n_steps:
            type: "categorical"
            choices: [2048, 4096, 8192]

        # Batch size for training
        batch_size:
            type: "categorical"
            choices: [64, 128, 256]

        # Number of epochs when optimizing the surrogate loss
        n_epochs:
            type: "int"
            low: 5
            high: 20

        # Discount factor for future rewards
        gamma:
            type: "float"
            low: 0.99
            high: 0.9999

        # Clipping parameter for PPO
        clip_range:
            type: "float"
            low: 0.1
            high: 0.3
